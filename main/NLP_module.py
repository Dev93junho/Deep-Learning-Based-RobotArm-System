# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rhls_ZjCmVQUG8iHYawWHJN1nJI6U15s
    
updated file dir in Dec.2021
"""

# !pip install konlpy # run this code If you run this file in colab
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.optim as optim
import torch.nn.functional as F

from konlpy.tag import Kkma
kor_tagger = Kkma()


"""
DATA Part will be create other *.json file
"""

# sample data
train_data = [["이것 좀 잡아줘","GRAB"],
                    ["이것 좀 잡아봐","GRAB"],
                    ["야 잡아","GRAB"],
                    ["이거","GRAB"],
                    ["저것 좀 잡아봐","GRAB"],
                    ["이것 좀 저어줘", "TOOL"],
                    ["젓고 있어", "TOOL"],
                    ["휘저어줘","TOOL"]]
                    

test_data = [["잡아봐","GRAB"],
                   ["야 이거 잡아","GRAB"],
                   ["이거 잡아","GRAB"]
                   ["저어봐","TOOL"],
                   ["계속 젓고 있어"]]


# preprocessing
train_X,train_y = list(zip(*train_data))
train_X = [kor_tagger.morphs(x) for x in train_X] # Tokenize
train_X

word2index={'<unk>' : 0}
for x in train_X:
    for token in x:
        if word2index.get(token)==None:
            word2index[token]=len(word2index)
            
class2index = {'GRAB' : 0, 'TOOL' : 1}
# print(word2index)
# print(class2index)

len(word2index)

word2index.get("패스트")

def make_BoW(seq,word2index):
    tensor = torch.zeros(len(word2index))
    for w in seq:
        index = word2index.get(w)
        if index!=None:
            tensor[index]+=1.
        else:
            index = word2index['<unk>']
            tensor[index]+=1.
    
    return tensor

train_X = torch.cat([Variable(make_BoW(x,word2index)).view(1,-1) for x in train_X])
train_y = torch.cat([Variable(torch.LongTensor([class2index[y]])) for y in train_y])

# print(train_X.size())

# train_X.size()

class BoWClassifier(nn.Module):
    def __init__(self,vocab_size,output_size):
        super(BoWClassifier,self).__init__()
        
        self.linear = nn.Linear(vocab_size,output_size)
    
    def forward(self,inputs):
        return self.linear(inputs)

# Train
STEP = 100
LR = 0.1
model = BoWClassifier(len(word2index),2)
loss_function = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(),lr=LR)

for step in range(STEP):
    model.zero_grad()
    predict = model(train_X)
    loss= loss_function(predict,train_y)
    if step % 10 == 0:
        print(loss.item())
    loss.backward()
    optimizer.step()

# Test
index2class = {v:k for k,v in class2index.items()}

for test in test_data:
    X = kor_tagger.nouns(test[0])
    X = Variable(make_BoW(X,word2index)).view(1,-1)
    
    pred = model(X)
    pred = pred.max(1)[1].item()
    print("Input : %s" % test[0])
    print("Prediction : %s" % index2class[pred])
    print("Truth : %s" % test[1])
    print("\n")
